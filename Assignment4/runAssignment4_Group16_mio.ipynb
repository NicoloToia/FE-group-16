{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Group 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the indexes as dictionary of {ticker: name}\n",
    "with open('data/_indexes.csv', 'r') as f:\n",
    "    # skip the first line\n",
    "    indexes = {\n",
    "        line.split(',')[1]: line.split(',')[2].strip()\n",
    "        for line in f.readlines()[1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual dataset as a dataframe\n",
    "EuroStoxx50 = pd.read_csv('data/EUROSTOXX50_Dataset.csv', sep=',', index_col=0, parse_dates=True)\n",
    "EuroStoxx50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the dataset\n",
    "EuroStoxx50.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that only contain NaN\n",
    "EuroStoxx50 = EuroStoxx50.dropna(axis=1, how='all')\n",
    "# for those who have NaN, fill them with the previous value\n",
    "EuroStoxx50 = EuroStoxx50.ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the log-returns dataframe\n",
    "returns = np.log(EuroStoxx50/EuroStoxx50.shift(1))\n",
    "returns = returns.dropna(axis=0, how='all')\n",
    "\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 0: Variance-Covariance method for VaR and ES in a linear portfolio\n",
    "\n",
    "On the 20th of February 2020 we have an equally weighted portfolio made up of the following equities\n",
    "\n",
    "- Adidas\n",
    "- Allianz\n",
    "- Munich Re\n",
    "- L'Oréal\n",
    "\n",
    "We compute the daily VaR and ES with a 5y estimation using a t-student distribution with 4 degrees\n",
    "of freedom ($\\nu$).\n",
    "The notional of the portfolio is 15 million €. We take a significance level of $\\alpha = 0.99\\%$.\n",
    "\n",
    "Wherever we have missing data due to differing trading days for each stock we substitute the previous\n",
    "day's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the relevant time series\n",
    "df = returns[['ADSGn.DE', 'ALVG.DE', 'MUVGn.DE', 'OREP.PA']]\n",
    "# set the date to 20th February 2020\n",
    "valuation_date = datetime(2020, 2, 20)\n",
    "# only use data prior to the valuation date\n",
    "df = df[df.index < valuation_date]\n",
    "# only use the last 5 years\n",
    "df = df[df.index > valuation_date - pd.DateOffset(years=5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nu and alpha\n",
    "nu = 4\n",
    "alpha = 0.99\n",
    "notional = 15 * 10**6\n",
    "\n",
    "# estimate the mean vector\n",
    "mean_df = df.mean()\n",
    "# estimate the covariance matrix\n",
    "Cov_df = df.cov()\n",
    "# create the weights vector\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily VaR\n",
    "We compute the daily VaR using the variance-covariance method. The daily VaR is given by:\n",
    "\n",
    "$$\n",
    "VaR_{\\alpha} = \\underbrace{\\bar\\mu \\cdot \\bar\\omega}_{\\mu} + \\underbrace{ \\sqrt{\\bar\\omega^T \\Sigma \\bar\\omega}}_{\\sigma} \\cdot t^{-1}_{\\nu} (\\alpha)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar\\omega$ is the vector of weights of the portfolio\n",
    "- $\\bar\\mu$ is the vector of expected returns of the portfolio\n",
    "- $\\Sigma$ is the variance-covariance matrix of the returns of the portfolio\n",
    "- $t^{-1}_{\\nu}(\\alpha)$ is the $\\alpha$-quantile of the t-student distribution with $\\nu$ degrees of freedom\n",
    "\n",
    "To compute the quantity $t^{-1}_{\\nu}(\\alpha)$ we use the `t.ppf` function from the `scipy.stats` module.\n",
    "\n",
    "See [this stackoverflow answer](https://stackoverflow.com/questions/65468026/norm-ppf-vs-norm-cdf-in-pythons-scipy-stats)\n",
    "and [this documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#:~:text=ppf(q%2C%20df,cdf%20%E2%80%94%20percentiles).) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the t_alpha quantile\n",
    "t_alpha = t.ppf(alpha, nu)\n",
    "\n",
    "# compute the VaR\n",
    "VaR = mean_df @ weights + np.sqrt(weights @ Cov_df @ weights) * t_alpha\n",
    "\n",
    "print(f'The daily VaR at 99% confidence level is {VaR:.2%}')\n",
    "print(f'The daily VaR at 99% confidence level is {VaR * notional:.2f} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily ES\n",
    "\n",
    "We compute the daily ES using the variance-covariance method. The daily ES is given by:\n",
    "\n",
    "$$\n",
    "ES_{\\alpha} = \\bar\\mu \\cdot \\bar\\omega +\n",
    "    \\sqrt{\\bar\\omega^T \\Sigma \\bar\\omega} \\cdot\n",
    "    \\underbrace{\n",
    "        \\frac{\\nu + ( t^{-1}_{\\nu}(\\alpha) )^2}{\\nu - 1} \\cdot \\frac{ \\phi_{\\nu} (t^{-1}_{\\nu}(\\alpha)) }{1 - \\alpha}\n",
    "    }_{ES_{\\alpha}^{std}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar\\omega$ is the vector of weights of the portfolio\n",
    "- $\\bar\\mu$ is the vector of expected returns of the portfolio\n",
    "- $\\Sigma$ is the variance-covariance matrix of the returns of the portfolio\n",
    "- $t^{-1}_{\\nu}(\\alpha)$ is the $\\alpha$-quantile of the t-student distribution with $\\nu$ degrees of freedom\n",
    "- $\\phi_{\\nu} (\\cdot)$ is the density function of the t-student distribution with $\\nu$ degrees of freedom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ES for the standard t-distribution\n",
    "ES_std = (nu + t_alpha**2) / (nu - 1) * (t.pdf(t_alpha, nu) / (1 - alpha))\n",
    "\n",
    "# compute the ES for the portfolio\n",
    "ES = mean_df @ weights + np.sqrt(weights @ Cov_df @ weights) * ES_std\n",
    "\n",
    "print(f'The daily ES at 99% confidence level is {ES:.2%}')\n",
    "print(f'The daily ES at 99% confidence level is {ES * notional:.2f} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 1: Historical simulation, bootstrap and PCA for VaR and ES in a linear portfolio\n",
    "\n",
    "On the 20th of March 2019 we must compute the following quantitities with $\\alpha = 0.95\\%$:\n",
    "\n",
    "- Portfolio 1: AXA (20K shares), Sanofi (20K shares), Volkswagen (10K shares).\n",
    "    We compute the daily VaR and ES with a Historical Simulation and Bootstrap method (with 200 simulations) and a 5 years estimation.\n",
    "- Portfolio 2: Adidas, Airbus, BBVA, BMW and Deutsche Telekom (all equally weighted).\n",
    "    We compute the daily VaR and ES with a 5 year estimation using a Weighted Historical Simulation with $\\lambda = 0.95$.\n",
    "- Portfolio 3: An equally weighted portfolio with shares of the first 18 companies.\n",
    "    We compute the 10 days VaR and ES with a 5 year estimation using a Gaussian parametric PCA approach using the first n principanl components (with n = 1, 2, 3, 4, 5).\n",
    "\n",
    "For each portfolio we also check the Plausibility Check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "We set the parameters for the various models and select the data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "alpha = 0.95\n",
    "lmd = 0.95 # lambda is a reserved keyword\n",
    "# set the valuation date to 20th March 2019\n",
    "valuation_date = datetime(2019, 3, 20)\n",
    "\n",
    "# select only the relevant returns\n",
    "df = returns[returns.index <= valuation_date]\n",
    "# only use the last 5 years\n",
    "df = df[df.index >= valuation_date - pd.DateOffset(years=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1.1 Portfolio 1\n",
    "First of all we set up the weights we will use to compute the various quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_shares = 50\n",
    "w_1 = np.array([20/tot_shares, 20/tot_shares, 10/tot_shares])\n",
    "indexes.get('AXA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Simulation\n",
    "We freeze the portfolio to the valutaion date and write the loss function as a function of the weights and the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the value of the portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an equally weighted portfolio\n",
    "w_2 = 0.20\n",
    "# create the array with the weights\n",
    "ptf_weights = np.array([w_2, w_2, w_2, w_2, w_2])\n",
    "# normalization factor\n",
    "C  = (1 - lmd) / (1 - lmd**len(df))\n",
    "# compute the decreasing sequence of weights: w_s = C*lambda^(t-s)\n",
    "weights = np.array([C * lmd**(len(df) - t) for t in range(len(df))])\n",
    "\n",
    "\n",
    "# extract the log returns of the following companies Adidas, Airbus, BBVA, BMW, Deutsche \n",
    "tickers_of_interest = ['ADSGn.DE', 'AIR.PA', 'BBVA.MC', 'BMWG.DE', 'DTEGn.DE']\n",
    "\n",
    "# Selecting columns for tickers of interest\n",
    "selected_returns = df[tickers_of_interest]\n",
    "\n",
    "selected_returns.head()\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the loss distribution\n",
    "loss  = - 1 * (selected_returns.T @ weights)\n",
    "\n",
    "# sort the loss in ascending order\n",
    "loss_sorted = sorted(loss, reverse=True)\n",
    "\n",
    "print(loss_sorted)\n",
    "\n",
    "#var = loss_sorted[int(len(loss_sorted) * alpha)]\n",
    "var = max([\n",
    "    loss_sorted[i]\n",
    "    for i in range(len(loss_sorted))\n",
    "    if np.sum(weights[:i]) <= 1 - alpha\n",
    "])\n",
    "\n",
    "print(var)\n",
    "\n",
    "print(f'The VaR at 95% confidence level is {var:.2%}')\n",
    "\n",
    "es = np.sum(loss_sorted) / np.sum(weights)\n",
    "\n",
    "print(f'The ES at 95% confidence level is {es:.2%}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
