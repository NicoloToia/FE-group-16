{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Group 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "from statsmodels.multivariate.pca import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our custom functions\n",
    "from Assignment4_lib import HSMeasurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the indexes as dictionary of {ticker: name}\n",
    "with open('data/_indexes.csv', 'r') as f:\n",
    "    # skip the first line\n",
    "    indexes = {\n",
    "        line.split(',')[1]: line.split(',')[2].strip()\n",
    "        for line in f.readlines()[1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual dataset as a dataframe\n",
    "EuroStoxx50 = pd.read_csv('data/EUROSTOXX50_Dataset.csv', sep=',')\n",
    "EuroStoxx50 = EuroStoxx50.set_index(pd.DatetimeIndex(EuroStoxx50['Date']))\n",
    "EuroStoxx50 = EuroStoxx50.drop('Date', axis=1)\n",
    "EuroStoxx50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the dataset\n",
    "EuroStoxx50.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that only contain NaN\n",
    "EuroStoxx50 = EuroStoxx50.dropna(axis=1, how='all')\n",
    "# for those who have NaN, fill them with the previous value\n",
    "EuroStoxx50 = EuroStoxx50.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the log-returns dataframe\n",
    "returns = np.log(EuroStoxx50/EuroStoxx50.shift(1))\n",
    "returns = returns.dropna(axis=0, how='all')\n",
    "\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 0: Variance-Covariance method for VaR and ES in a linear portfolio\n",
    "\n",
    "On the 20th of February 2020 we have an equally weighted portfolio made up of the following equities\n",
    "\n",
    "- Adidas\n",
    "- Allianz\n",
    "- Munich Re\n",
    "- L'Oréal\n",
    "\n",
    "We compute the daily VaR and ES with a 5y estimation using a t-student distribution with 4 degrees\n",
    "of freedom ($\\nu$).\n",
    "The notional of the portfolio is 15 million €. We take a significance level of $\\alpha = 0.99\\%$.\n",
    "\n",
    "Wherever we have missing data due to differing trading days for each stock we substitute the previous\n",
    "day's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the relevant time series\n",
    "df = returns[['ADSGn.DE', 'ALVG.DE', 'MUVGn.DE', 'OREP.PA']]\n",
    "# set the date to 20th February 2020\n",
    "valuation_date = datetime(2020, 2, 20)\n",
    "# only use data prior to the valuation date\n",
    "df = df[df.index < valuation_date]\n",
    "# only use the last 5 years\n",
    "df = df[df.index >= valuation_date - pd.DateOffset(years=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nu and alpha\n",
    "nu = 4\n",
    "alpha = 0.99\n",
    "notional = 15 * 10**6\n",
    "\n",
    "# estimate the mean vector\n",
    "mean_df = df.mean()\n",
    "# estimate the covariance matrix\n",
    "Cov_df = df.cov()\n",
    "# create the weights vector\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily VaR\n",
    "We compute the daily VaR using the variance-covariance method. The daily VaR is given by:\n",
    "\n",
    "$$\n",
    "VaR_{\\alpha} = \\underbrace{\\bar\\mu \\cdot \\bar\\omega}_{\\mu} + \\underbrace{ \\sqrt{\\bar\\omega^T \\Sigma \\bar\\omega}}_{\\sigma} \\cdot t^{-1}_{\\nu} (\\alpha)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar\\omega$ is the vector of weights of the portfolio\n",
    "- $\\bar\\mu$ is the vector of expected returns of the portfolio\n",
    "- $\\Sigma$ is the variance-covariance matrix of the returns of the portfolio\n",
    "- $t^{-1}_{\\nu}(\\alpha)$ is the $\\alpha$-quantile of the t-student distribution with $\\nu$ degrees of freedom\n",
    "\n",
    "To compute the quantity $t^{-1}_{\\nu}(\\alpha)$ we use the `t.ppf` function from the `scipy.stats` module.\n",
    "\n",
    "See [this stackoverflow answer](https://stackoverflow.com/questions/65468026/norm-ppf-vs-norm-cdf-in-pythons-scipy-stats)\n",
    "and [this documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#:~:text=ppf(q%2C%20df,cdf%20%E2%80%94%20percentiles).) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the t_alpha quantile\n",
    "t_alpha = t.ppf(alpha, nu)\n",
    "\n",
    "# compute the VaR\n",
    "VaR = mean_df @ weights + np.sqrt(weights @ Cov_df @ weights) * t_alpha\n",
    "\n",
    "print(f'The daily VaR at 99% confidence level is {VaR:.2%}')\n",
    "print(f'The daily VaR at 99% confidence level is {VaR * notional:.2f} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily ES\n",
    "\n",
    "We compute the daily ES using the variance-covariance method. The daily ES is given by:\n",
    "\n",
    "$$\n",
    "ES_{\\alpha} = \\bar\\mu \\cdot \\bar\\omega +\n",
    "    \\sqrt{\\bar\\omega^T \\Sigma \\bar\\omega} \\cdot\n",
    "    \\underbrace{\n",
    "        \\frac{\\nu + ( t^{-1}_{\\nu}(\\alpha) )^2}{\\nu - 1} \\cdot \\frac{ \\phi_{\\nu} (t^{-1}_{\\nu}(\\alpha)) }{1 - \\alpha}\n",
    "    }_{ES_{\\alpha}^{std}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar\\omega$ is the vector of weights of the portfolio\n",
    "- $\\bar\\mu$ is the vector of expected returns of the portfolio\n",
    "- $\\Sigma$ is the variance-covariance matrix of the returns of the portfolio\n",
    "- $t^{-1}_{\\nu}(\\alpha)$ is the $\\alpha$-quantile of the t-student distribution with $\\nu$ degrees of freedom\n",
    "- $\\phi_{\\nu} (\\cdot)$ is the density function of the t-student distribution with $\\nu$ degrees of freedom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ES for the standard t-distribution\n",
    "ES_std = (nu + t_alpha**2) / (nu - 1) * (t.pdf(t_alpha, nu) / (1 - alpha))\n",
    "\n",
    "# compute the ES for the portfolio\n",
    "ES = mean_df @ weights + np.sqrt(weights @ Cov_df @ weights) * ES_std\n",
    "\n",
    "print(f'The daily ES at 99% confidence level is {ES:.2%}')\n",
    "print(f'The daily ES at 99% confidence level is {ES * notional:.2f} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 1: Historical simulation, bootstrap and PCA for VaR and ES in a linear portfolio\n",
    "\n",
    "On the 20th of March 2019 we must compute the following quantitities with $\\alpha = 0.95\\%$:\n",
    "\n",
    "- Portfolio 1: Total (25K shares), AXA (20K shares), Sanofi (20K shares), Volkswagen (10K shares).\n",
    "    We compute the daily VaR and ES with a Historical Simulation and Bootstrap method (with 200 simulations) and a 5 years estimation.\n",
    "- Portfolio 2: Adidas, Airbus, BBVA, BMW and Deutsche Telekom (all equally weighted).\n",
    "    We compute the daily VaR and ES with a 5 year estimation using a Weighted Historical Simulation with $\\lambda = 0.95$.\n",
    "- Portfolio 3: An equally weighted portfolio with shares of the first 18 companies.\n",
    "    We compute the 10 days VaR and ES with a 5 year estimation using a Gaussian parametric PCA approach using the first n principanl components (with n = 1, 2, 3, 4, 5).\n",
    "\n",
    "For each portfolio we also check the Plausibility Check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "We set the parameters for the various models and select the data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "alpha = 0.95\n",
    "lmd = 0.94 # lambda is a reserved keyword\n",
    "# set the valuation date to 20th March 2019\n",
    "valuation_date = datetime(2019, 3, 20)\n",
    "\n",
    "# select only the relevant returns\n",
    "df = returns[returns.index <= valuation_date]\n",
    "# only use the last 5 years\n",
    "df = df[df.index >= valuation_date - pd.DateOffset(years=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1.1 Portfolio 1\n",
    "First of all we set up the weights we will use to compute the various quantities.\n",
    "Recall that portfolio 1 has the following quantities:\n",
    "- Total (25K shares)\n",
    "- AXA (20K shares)\n",
    "- Sanofi (20K shares)\n",
    "- Volkswagen (10K shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant indexes\n",
    "df_1_1 = df[['TTEF.PA', 'AXAF.PA', 'SASY.PA', 'VOWG_p.DE']]\n",
    "# compute the value at valuation date\n",
    "val_Total = 25_000 * EuroStoxx50.loc[valuation_date]['TTEF.PA']\n",
    "val_AXA = 20_000 * EuroStoxx50.loc[valuation_date]['AXAF.PA']\n",
    "val_Sanofi = 20_000 * EuroStoxx50.loc[valuation_date]['SASY.PA']\n",
    "val_VW = 10_000 * EuroStoxx50.loc[valuation_date]['VOWG_p.DE']\n",
    "V_t = val_Total + val_AXA + val_Sanofi + val_VW\n",
    "# compute the weights\n",
    "weights = np.array([val_Total, val_AXA, val_Sanofi, val_VW]) / V_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Simulation approach\n",
    "\n",
    "In order to apply the Historical Simulation approach we need to compute the losses for each day as:\n",
    "\n",
    "$$\n",
    "L_t = - V_t \\cdot \\omega \\cdot r_t\n",
    "$$\n",
    "\n",
    "then we sort the losses in descending order (the value of index 0 is the highest loss) and we take\n",
    "the $\\alpha$-quantile of the historical losses as $q_{\\alpha, HS} = \\lfloor (1 - \\alpha) \\cdot N \\rfloor$, with \n",
    "$N$ the number of observations.\n",
    "\n",
    "Then we simply take:\n",
    "\n",
    "$$\n",
    "\n",
    "VaR_{\\alpha} =  L^{(q_{\\alpha, HS}, N)} \\\\\n",
    "\n",
    "ES_{\\alpha} = \\frac{1}{q_{\\alpha, HS}} \\sum_{i=1}^{q_{\\alpha, HS}} L^{(i, N)}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES, VaR = HSMeasurements(df_1_1, alpha, weights, V_t, 1)\n",
    "\n",
    "print(f\"The daily VaR at 95% confidence level is {VaR:.2f} EUR\")\n",
    "print(f\"The daily ES at 95% confidence level is {ES:.2f} EUR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1.3: Portfolio 3\n",
    "\n",
    "We compute the 10 days VaR and ES with a 5 year estimation using a Gaussian parametric PCA approach using the first n principanl components (with n = 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "\n",
    "We set up this portfolio by selecting the first 18 companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 18 companies without any NaN values\n",
    "stocks_1_3 = df.dropna(axis=1, how='any').iloc[:, :18]\n",
    "stocks_1_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA approach\n",
    "\n",
    "We compute the PCA on the returns for each n = 1, 2, 3, 4, 5 and use the first n principal components to compute the VaR and ES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the PCA for each n\n",
    "pcas = {\n",
    "    n: PCA(stocks_1_3, ncomp=n, normalize=True)\n",
    "    for n in range(1, 6)\n",
    "}\n",
    "# extract the coefficients from each PCA\n",
    "coeffs = {\n",
    "    n: pcas[n].coeff\n",
    "    for n in pcas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # sum by ticker\n",
    "    n: np.sum(coeffs[n], axis=0)\n",
    "    for n in coeffs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean vector and covariance matrix\n",
    "mu = stocks_1_3.mean()\n",
    "cov = stocks_1_3.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean as the scalar product of the weights and the mean vector\n",
    "mus = {\n",
    "    n: np.dot(weights[n], mu)\n",
    "    for n in weights\n",
    "}\n",
    "\n",
    "sigmas = {\n",
    "    n: np.dot(np.dot(weights[n], cov), weights[n])\n",
    "    for n in weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the VaR for each n using a standard Gaussian distribution\n",
    "VaRs = {\n",
    "    n: mus[n] + np.sqrt(sigmas[n]) * norm.ppf(alpha)\n",
    "    for n in mus\n",
    "}\n",
    "\n",
    "# compute the ES for each n using a standard Gaussian distribution\n",
    "ESs = {\n",
    "    n: mus[n] + np.sqrt(sigmas[n]) * norm.pdf(norm.ppf(alpha)) / (1 - alpha)\n",
    "    for n in mus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "for n in VaRs:\n",
    "    print(f'For n = {n}, the VaR at 95% confidence level is {VaRs[n]:.2%}')\n",
    "    print(f'For n = {n}, the ES at 95% confidence level is {ESs[n]:.2%}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
